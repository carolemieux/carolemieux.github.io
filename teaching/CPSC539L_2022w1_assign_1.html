<!DOCTYPE html>

<html>

<head>
  <link rel="apple-touch-icon" sizes="180x180" href="../apple-touch-icon.png">
  <link rel="icon" type="image/png" sizes="32x32" href="../favicon-32x32.png">
  <link rel="icon" type="image/png" sizes="16x16" href="../favicon-16x16.png">
  <link rel="manifest" href="../site.webmanifest">
  <link rel="stylesheet" type="text/css" href="../main.css">
  <link rel="stylesheet" type="text/css" href="assign_code_format.css">
  <link rel="stylesheet" href="https://fonts.googleapis.com/css?family=Open+Sans">
  <link rel="stylesheet" href="https://fonts.googleapis.com/css?family=Inconsolata">
  <link href="https://fonts.googleapis.com/css2?family=Open+Sans:wght@700&display=swap" rel="stylesheet">
</head>

<body>

  <div class="main">

    <title>2022 W1 CPSC 539L: Assignment 1 </title>

    <table id="main">

      <tr>
        <td style="padding-bottom: 20px">
          <h1>2022 W1 CPSC 539L: Assignment 1</h1>

          <p style="margin-bottom:-8px; color:gray"><small>CPSC 539L, Winter Term 1 2022, Instructor: Caroline Lemieux
              (clemieux@cs.ubc.ca)</small></p>
          <p style="margin-bottom:-8px;color:gray"><small>Mon/Wed 10:30AM-12:00PM, DMP 101, <a
                href="https://courses.students.ubc.ca/cs/courseschedule?sesscd=W&pname=subjarea&tname=subj-section&course=539L&sessyr=2022&section=101&dept=CPSC">UBC
                course page</a></small></p>
          <p style="margin-bottom:-8px; color:gray"><small>Back to <a href="CPSC539L_2022w1.html">main course
                webpage</a></small></p>

          </br>

        </td>
      </tr>


      <tr>
        <td>

          <p><b>Quick jump:</b> <a href="#background">Background</a> * <a href="#instructions">Instructions</a> * <a
              href="#submission">Submission</a></p>

          <p>In this assignment you are provided with the skeleton of a random and coverage-guided fuzz testing tool for
            small Python programs. This assignment has a programming component in which you will implement the core
            heuristics of the fuzz testing tools. It also has an experimental component in which evaluate the tools you
            have built on some small benchmark programs. For the experimental component, it is recommended that you
            run each designated technique on each benchmark 3 times, for 1 minute each. This means a total of about 20
            minutes of experiment runtime if all experiments are run sequentially.</p>

          <h3>Learning Goals</h3>

          <ul>
            <li>Construct heuristics for the core of a random and a coverage-guided fuzzer</li>
            <li>Identify when coverage-guided fuzzing might yield better performance than random fuzzing and vice-versa
            </li>
            <li>Analyze the effect of input seeds on the performance of coverage-guided fuzzing</li>
            <li>Compare the performance of structured fuzzing with different levels of structure specification</li>
          </ul>

          <h3>Questions</h3>
          <p>Ask questions, report potential typos, etc., about the assignment on the <a
              href="https://piazza.com/class/l70sge6r44k4xd/">public class
              Piazza</a> as much as possible so other students can benefit from the clarifications.</p>

          <h1 id="background">Background</h1>

          <p>In class we will read two papers which cover the basics of random, coverage-guided, and generator-based
            fuzzing. The September 14th lecture will also cover these topics. The following recaps the core background.
          </p>

          <h4>Random Fuzzing</h4>

          <p><i>See also <a href="https://dl.acm.org/doi/pdf/10.1145/96267.96279">the description of "fuzz"</a> in the
              original fuzzing paper.</i> A random fuzzer simply produces totally random inputs and sends these to the
            program
            under test for the duration of the fuzzing session.</p>

          <p>The pseudo-code for random fuzzing is as follows:
          <div class="highlight">
            <pre><span></span><span class="k">def</span> <span class="nf">random_fuzz</span><span class="p">(</span><span class="n">program_to_test</span><span class="p">,</span> <span class="n">max_search_time</span><span class="p">):</span>
    <span class="n">start_time</span> <span class="o">=</span> <span class="n">current_time</span><span class="p">()</span>
    <span class="k">while</span> <span class="p">(</span><span class="n">current_time</span><span class="p">()</span> <span class="o">-</span> <span class="n">start_time</span> <span class="o">&lt;</span> <span class="n">max_search_time</span><span class="p">):</span>
        <span class="n">new_input</span> <span class="o">=</span> <span class="n">generate_random_input</span><span class="p">()</span>
        <span class="n">has_error</span><span class="p">,</span> <span class="n">input_coverage</span> <span class="o">=</span> <span class="n">execute_on_input</span><span class="p">(</span><span class="n">program_to_test</span><span class="p">,</span> <span class="n">new_input</span><span class="p">)</span>
        <span class="k">if</span> <span class="p">(</span><span class="n">has_error</span><span class="p">):</span>
            <span class="n">save_as_error</span><span class="p">(</span><span class="n">new_input</span><span class="p">)</span>
</pre>
          </div>
          It is not necessary to keep track of coverage achieved while performing random fuzzing. Our implementation
          does this is so you can compare its coverage to that of coverage-guided fuzzing (see the discussion in ``Bonus
          Exploration''). </p>

          <h4>Coverage-Guided Fuzzing</h4>

          <p>See also <a href="https://www.carolemieux.com/zest_issta19.pdf">Section 2.2</a> of the Zest paper.
            The core algorithm of coverage-guided fuzzing you will be implementing is the following:

          <div class="highlight">
            <pre><span></span><span class="k">def</span> <span class="nf">coverage_guided_fuzz</span><span class="p">(</span><span class="n">program_under_test</span><span class="p">,</span> <span class="n">seed_inputs</span><span class="p">,</span> <span class="n">max_search_time</span><span class="p">):</span>
    <span class="n">start_time</span> <span class="o">=</span> <span class="n">current_time</span><span class="p">()</span>
    <span class="n">saved_inputs</span> <span class="o">=</span> <span class="nb">set</span><span class="p">(</span><span class="n">seed_inputs</span><span class="p">)</span>
    <span class="k">while</span> <span class="p">(</span><span class="n">current_time</span><span class="p">()</span> <span class="o">-</span> <span class="n">start_time</span> <span class="o">&lt;</span> <span class="n">max_search_time</span><span class="p">):</span>
       <span class="k">for</span> <span class="n">saved_input</span> <span class="ow">in</span> <span class="n">saved_inputs</span><span class="p">:</span>
          <span class="k">if</span> <span class="n">random_float</span><span class="p">()</span> <span class="o">&lt;</span> <span class="n">FUZZ_PROB</span><span class="p">(</span><span class="n">saved_input</span><span class="p">):</span>
             <span class="n">num_mutants</span> <span class="o">=</span> <span class="n">NUM_MUTANTS</span><span class="p">(</span><span class="n">saved_input</span><span class="p">)</span>
             <span class="k">for</span> <span class="n">_</span> <span class="ow">in</span> <span class="nb">range</span><span class="p">(</span><span class="mi">0</span><span class="p">,</span> <span class="n">num_mutants</span><span class="p">):</span>
                <span class="n">mutated_input</span> <span class="o">=</span> <span class="n">MUTATE_INPUT</span><span class="p">(</span><span class="n">saved_input</span><span class="p">)</span>
                <span class="n">has_error</span><span class="p">,</span> <span class="n">input_coverage</span>  <span class="o">=</span> <span class="n">execute_on_input</span><span class="p">(</span>
                      <span class="n">mutated_input</span>
                <span class="p">)</span>
                <span class="k">if</span> <span class="p">(</span><span class="n">has_error</span><span class="p">):</span>
                   <span class="n">save_as_error</span><span class="p">(</span><span class="n">mutated_input</span><span class="p">)</span>
                <span class="k">elif</span> <span class="p">(</span><span class="n">has_new_coverage</span><span class="p">(</span><span class="n">input_coverage</span><span class="p">)):</span>
                   <span class="n">saved_inputs</span><span class="o">.</span><span class="n">add</span><span class="p">(</span><span class="n">mutated_input</span><span class="p">)</span>
 
</pre>
          </div>
          The real implementation in <code>fuzzer.py</code> includes some additional statements for statistics tracking
          purposes, as well as looping over <code>saved_inputs</code> that is safe to the expansion of
          <code>saved_inputs</code>.</p>

          <p>In a nutshell, the algorithm starts from a set of seed inputs (or some arbitrarily-generated input if no
            seed input is provided). These populate the set <code>saved_input</code>. Coverage-guided fuzzing produces
            new inputs by repeatedly mutating the inputs in <code>saved_inputs</code>. Any new mutated inputs which are
            found to increase code coverage are added to <code>saved_inputs</code>, and the process repeats.</p>

          <p>Three core heuristics control the performance of coverage-guided fuzzing:</p>

          <ul>
            <li>We may not always want to mutate all the inputs in <code>saved_inputs</code>, especially as the size of
              <code>saved_inputs</code> grows. <code>FUZZ_PROB</code>, aka the <code>fuzz_prob</code> method in
              <code>CoverageGuidedFuzzer</code>, controls the likelihood of selecting an input in
              <code>saved_inputs</code> for mutation.
            </li>
            <li>Mutating + running mutated inputs is time-consuming. So we may not want to produce as many mutated
              inputs from each input selected for mutation. <code>NUM_MUTANTS</code>, aka the <code>num_mutants</code>
              method in <code>CoverageGuidedFuzzer</code>, controls how many mutants we will generate from a given
              parent input.</li>
            <li>Above we refer to the process of mutating inputs, but how exactly should this be done? Small mutations
              allow targeted exploration near a particular input; larger mutations may allow for faster exploration of
              different behaviors. <code>MUTATE_INPUT</code>, aka the <code>mutate_input</code> method in
              <code>CoverageGuidedFuzzer</code> defines how to mutate a particular input. This method should certainly
              be non-deterministic.
            </li>
          </ul>

          <p>In practice, other aspects of the algorithm, such as deciding when to save an input, can also affect the
            performance of
            fuzzing. For the purposes of this assignment, we will consider these fixed.</p>

          <h4>Generator-Based Fuzzing</h4>

          <p>See also <a href="https://www.carolemieux.com/zest_issta19.pdf">Section 2.1</a> of the Zest paper.</p>

          <p>At its conceptual core, generator-based fuzzing is simply random fuzzing, where, rather than generating a
            random input from scratch, a provided input generator is called to generate inputs:</p>

          <div class="highlight">
            <pre><span></span><span class="k">def</span> <span class="nf">generator_fuzz</span><span class="p">(</span><span class="n">program_to_test</span><span class="p">,</span> <span class="n">max_search_time</span><span class="p">,</span> <span class="n"><b>input_generator</b></span><span class="p">):</span>
    <span class="n">start_time</span> <span class="o">=</span> <span class="n">current_time</span><span class="p">()</span>
    <span class="k">while</span> <span class="p">(</span><span class="n">current_time</span><span class="p">()</span> <span class="o">-</span> <span class="n">start_time</span> <span class="o">&lt;</span> <span class="n">max_search_time</span><span class="p">):</span>
        <span class="n">new_input</span> <span class="o">=</span> <span class="n"><b>input_generator</b></span><span class="p">()</span>
        <span class="n">has_error</span><span class="p">,</span> <span class="n">input_coverage</span> <span class="o">=</span> <span class="n">execute_on_input</span><span class="p">(</span><span class="n">program_to_test</span><span class="p">,</span> <span class="n">new_input</span><span class="p">)</span>
        <span class="k">if</span> <span class="p">(</span><span class="n">has_error</span><span class="p">):</span>
            <span class="n">save_as_error</span><span class="p">(</span><span class="n">new_input</span><span class="p">)</span>
</pre>
          </div>

          <p>You will not be implementing a generator-based fuzzer; the benchmarks
            <code>cgi_decode_good_generator.py</code> and <code>cgi_decode_ok_generator.py</code> build the generator
            into the test driver. For instance, the <code>test_one_input</code> function (i.e., the program under test)
            defined in
            <code>cgi_decode.py</code> is:
          </p>
          <div class="highlight">
            <pre><span></span><span class="k">def</span> <span class="nf">test_one_input</span><span class="p">(</span><span class="n">input_data</span><span class="p">:</span> <span class="nb">bytes</span><span class="p">):</span>
    <span class="k">try</span><span class="p">:</span>
        <span class="n">input_str</span> <span class="o">=</span> <span class="n">input_data</span><span class="o">.</span><span class="n">decode</span><span class="p">(</span><span class="s2">&quot;UTF-8&quot;</span><span class="p">)</span>
        <span class="n">cgi_decode</span><span class="p">(</span><span class="n">input_str</span><span class="p">)</span>
    <span class="k">except</span> <span class="ne">ValueError</span><span class="p">:</span>
        <span class="c1"># Invalid input, but not a bug</span>
        <span class="k">pass</span>
</pre>
          </div>



          <p>While in <code>cgi_decode_ok_generator.py</code> it is:</p>

          <div class="highlight">
            <pre><span></span><span class="k">def</span> <span class="nf">test_one_input</span><span class="p">(</span><span class="n">input_data</span><span class="p">:</span> <span class="nb">bytes</span><span class="p">):</span>
    <span class="k">try</span><span class="p">:</span>
        <span class="n">input_str</span> <span class="o">=</span> <span class="n">generate_cgi_string</span><span class="p">(</span><span class="n">input_data</span><span class="p">)</span>
        <span class="n">cgi_decode</span><span class="p">(</span><span class="n">input_str</span><span class="p">)</span>
    <span class="k">except</span> <span class="ne">ValueError</span><span class="p">:</span>
        <span class="c1"># Invalid input, but not a bug</span>
        <span class="k">pass</span>
</pre>
          </div>

          <p>The generator <code>generate_cgi_string</code> uses
            the provided sequence of bytes to build the input, rather than polling a random source to make decisions
            about input generation. Refer to the implementation in <code>generators/cgi_string.py</code> for how exactly
            this works. In the Zest paper, we refer to these generators as <i>parametric generators</i>.</p>


          <h4>Code Coverage</h4>

          <p><em>Line coverage</em> refers to the lines executed at least once by the program under test. <em>Branch
              coverage</em> refers to whether both "sides" of an <code>if</code> statement (or <code>while</code>
            statement, or <code>switch</code> statement...) are covered.</p>

          <p>For example, consider the program <code>foo</code> here:</p>

          <p>

          <div class="highlight">
            <pre><span></span><span class="k">def</span> <span class="nf">foo</span><span class="p">(</span><span class="n">x</span><span class="p">,</span> <span class="n">y</span><span class="p">):</span>
  <span class="n">z</span> <span class="o">=</span> <span class="mi">2</span> <span class="o">*</span> <span class="n">x</span>
  <span class="k">if</span> <span class="n">z</span> <span class="o">&gt;</span> <span class="n">y</span><span class="p">:</span>
      <span class="n">z</span> <span class="o">=</span> <span class="n">y</span>
  <span class="k">return</span> <span class="n">z</span> <span class="o">+</span> <span class="n">y</span>
</pre>
          </div>
          </p>

          <ul>
            <li>The call <code>foo(3,2)</code> gets full line-coverage of the program: all lines are executed. It does
              not get full branch coverage, since only the true side of <code>if z &gt; y:</code> is executed. </li>
            <li>The call <code>foo(3,7)</code> misses one line (<code>z = y</code>) so does not get full line coverage.
              It does not get full branch coverage, since only the false side of <code>if z &gt; y:</code> is executed.
            </li>
            <li>Together, the calls to <code>foo(3,2)</code> and <code>foo(3,7)</code> achieve full line + branch
              coverage of the program. </li>
          </ul>

          <p>In this assignment, we use the <a href="https://coverage.readthedocs.io/en/6.4.4/">coverage.py</a> library
            to measure coverage. The fuzzer only considers branch (aka "edge" or "arc") coverage as "new coverage", but
            the produced html coverage reports also show you the line coverage of the program under test.
            In the coverage.py html report, lines covered are highlighted green; lines not covered are highlighted red
            red. Lines containing a branch which is not fully covered (i.e., only the true or false case is covered, not
            both) are in yellow. </p>

          <h1 id="instructions">Instructions</h1>

          <h3>Collaboration policy</h3>

          <p>You may discuss the assignment with other students but all coding and the writeup should be done on your
            own. </p>

          <h3>Startup</h3>

          <p>You will need a machine that runs Python 3.8+. One external package is needed to run the fuzzing, tool,
            <code>coverage.py</code>. You should be able to install this with <code>pip</code> (either
            <code>pip install converage</code> or <code>pip3 install coverage</code> depending on your system); see
            instructions <a href="https://coverage.readthedocs.io/en/6.4.2/install.html">here</a>.
          </p>

          <p>Download the startup code here: <a
              href="cpsc539l-assignment-materials.zip">cpsc539l-assignment-materials.zip</a>. After unpacking the
            archive, you will be left with a
            directory with the following structure:</p>
          <ul>
            <li> <code>fuzzer.py</code>: this is the file in which you will complete your implementation tasks</li>
            <li> <code>benchmarks</code>: this folder contains several benchmarks on which you can run your fuzzer</li>
            <ul>
              <li> <code>quicksort.py</code>: an implementation of quicksort, you can use this to quickly run</li>
              <li> <code>cgi_decode.py</code>: a decoder of <a
                  href="https://en.wikipedia.org/wiki/Percent-encoding">CGI-encoded</a> strings, which accepts the
                inputs directly generated by the fuzzer.</li>
              <li> <code>cgi_decode_ok_generator.py</code>: a decoder of CGI-encoded strings, where the fuzzer-generated
                bytes are used to generate a <em>string with valid CGI characters</em>, which is then sent to the
                decoder. This is effectively a generator-based fuzzer with a very simple generator.</li>
              <li> <code>cgi_decode_good_generator.py</code>: a decoder of CGI-encoded strings, where the
                fuzzer-generated bytes are used to generate a <em>valid CGI string</em>, which is then sent to the
                decoder. This is effectively a generator-based fuzzer with a generator which only creates valid inputs
                for the CGI decoder. </li>
            </ul>
            <li> <code>generators</code>: this folder contains generators which convert a fuzzer-generated bytestring
              into more structured data</li>
            <ul>
              <li> <code>cgi_str.py</code>: contains the generator for valid CGI strings (used in the
                "<code>good</code>" benchmark), and the generator for strings
                with valid CGI characters (used in the "<code>ok</code>" benchmark).</li>
            </ul>
            <li> <code>seeds</code>: this folder contains an input file to seed coverage-guided fuzzing with on the
              <code>cgi_decode</code> benchmark</code></li>
          </ul>
          <p>The core code of the <code>cgi_decode</code> benchmark is taken from the <a
              href="https://www.fuzzingbook.org/html/Coverage.html">Fuzzing Book</a>.</p>

          <h3>Programming Component</h3>

          <p>You have <em>four</em> functions to fill in. After filling these functions in you should be able to run (a)
            your random fuzzer and (b) your coverage guided fuzzer on the provided benchmarks.</p>

          <ol>
            <li>
              <p>Random fuzzer: fill in the <code>generate_input</code> method in the <code>RandomFuzzer</code> class in
                order to complete the random fuzzer.
            </li>
            </p>
            <li>
              <p>Coverage-guided fuzzer: there are 3 methods to fill in the <code>CoverageGuidedFuzzer</code> class:</p>
              <ol type="a">
                <li> <code>fuzz_prob(input)</code>: given a saved <code>input</code>, should we use it as a parent for
                  mutation? If <code>fuzz_prob</code> returns 1, we will always use it as a parent for mutation; if it
                  returns 0, it will not be used as a parent for mutation.</li>
                <li> <code>num_mutants(input)</code>: given a saved <code>input</code> selected for mutation, how many
                  "children" (or "mutant") inputs should we create from it?</li>
                <li><code>mutate_input(input_data)</code>: given a byte-string input, create a new byte-string input
                  from
                  it. Note that this method should definitely be non-deterministic, i.e. not always return the same
                  mutant.</li>
              </ol>
            </li>
          </ol>

          <p>You can test your random fuzzer on the <code>quicksort</code> benchmark as follows:
          <pre> <code>
  $ python3 fuzzer.py benchmarks.quicksort &lt;OUTPUT_DIR_NAME&gt; --fuzzer RANDOM --time 10
    </code></pre>

          and you can test your coverage-guided fuzzer on the <code>quicksort</code> benchmark as follows:
          <pre><code>
  $ python3 fuzzer.py benchmarks.quicksort &lt;OUTPUT_DIR_NAME&gt; --fuzzer COVERAGE --time 10
              </code></pre>
          </p>

          <p>Describe the final implementation you chose for each of these four functions in your assignment writeup.
          </p>

          <h4>Hints</h4>
          <ul>
            <li>
              Our fuzzers produce Python bytestring inputs, rather than regular python strings. You can construct a
              python <code>bytes</code> object from a list of integers <code>lst</code>, where each <code>x</code> in
              the list is <code>0 &lt;= x &lt; 255</code>, with the function <code>bytes()</code>, i.e.:
              <code>bytes(lst)</code>.
            </li>
            <li>
              You need not use all the fields in the <code>SavedInput</code> and <code>Fuzzer</code> classes in your
              implementation; you may also track any other additional information in your implementation.
            </li>
            <li>
              There are no "right" implementations for any of these functions, espsecially those in
              <code>CoverageGuidedFuzzer</code>. However, the performance of your coverage-guided fuzzer may vary
              depending on your implementation. For instance, if you make <code>fuzz_prob</code> and
              <code>num_children</code> output a constant value, the performance may be more similar to a random
              fuzzer. Some heuristics you may consider:
              <ul>
                <li>Choosing to mutate (have a higher <code>fuzz_prob</code>) more recently-discovered inputs; inputs
                  that have not yet been mutated; inputs that cover more branches</li>
                <li>Choosing to generate more children for fast-executing and short (in length) inputs.</li>
              </ul>
            </li>
            <li>
              For more inspiration, you can check out the <a
                href="https://lcamtuf.coredump.cx/afl/technical_details.txt">technical whitepaper</a> for AFL, or
              consider the approaches in <a href="https://mboehme.github.io/paper/TSE18.pdf">different</a> <a
                href="https://www.carolemieux.com/fairfuzz-ase18.pdf">research</a> <a
                href="https://mboehme.github.io/paper/FSE20.Entropy.pdf">papers</a>.
            </li>
          </ul>

          <h3>Experimental Component.</h3>

          <p>Now that you have a running random fuzzer and coverage-guided fuzzer, you will evaluate them on a few
            benchmarks. For each benchmark and technique, run 3 1-minute experiments. You can run them sequentially or
            in parallel; make sure to specify different output directories for each repetition.</p>

          <ol>
            <li>
              <p>Run the random and coverage-guided fuzzer on the <code>cgi_decode</code> benchmark for 1 minute each.
              </p>
              <ol type="a">
                <li> What is the average branch coverage achieved by each technique? How long does it take to achieve
                  this maximum coverage? Does this vary across runs?</li>
                <li> Does each technique discover the crash? How long does it take for each technique to discover the
                  crash on average? Does this vary across runs?</li>
              </ol>
            </li>
            <li>
              <p>Run the coverage-guided fuzzer on the <code>cgi_decode</code> benchmark, this time <em>with</em> the
                input seeds from the <code>seeds/cgi_decode</code> directory, i.e.:
                <code>python3 fuzzer.py benchmarks.cgi_decode &lt;OUTPUT_DIR&gt;  --input_dir seeds/cgi_decode --fuzzer COVERAGE --time 60 
  </code>
              </p>
              <ol type="a">
                <li>What is the average branch coverage achieved by each technique? How long does it take to achieve
                  this maximum coverage? Does this vary across runs? How does it compare to the coverage-guided fuzzing
                  without seeds?
                <li>Does the technique discover crashes? How long does it take to discover each crash on
                  average? How does this compare to coverage-guided fuzzing without seeds?</li>
            </li>
          </ol>
          </li>
          <li>
            <p>Now run the random and coverage-guided fuzzer on the <code>cgi_decode_ok_generator</code> and
              <code>cig_decode_good_generator</code> benchmarks. These benchmarks to not pass the fuzzer-generated input
              directly to the inputs. Instead, they use the fuzzer-generated inputs as a source of randomness for an
              "ok" (more likely to generate valid CGI-format strings) and "good" generator (only generates valid
              CGI-format strings) of CGI-format strings.
            </p>
            <ol type="a">
              <li>Does the performance (coverage, time to find crashes) differ very much between random and
                coverage-guided fuzzing on these benchmarks? Why do you think the performance does/does not differ?
              </li>
              <li>How does the performance (coverage, time to find crashes) differ between the "ok" and "good"
                generator benchmarks?</li>
              <li>How does the performance (coverage, time to find crashes) of coverage-guided fuzzing on these
                benchmarks compare to coverage-guided fuzzing on <code>cgi_decode</code> without seeds (question 1)?
                Why do you think the performance does/does not differ?</li>
              <li>How does the performance (coverage, time to find crashes) compare to <code>cgi_decode</code>
                with seeds (question 2)? Why do you think the performance does/does not differ?</li>
            </ol>
          </li>
          </ol>

          <h1 id="submission">Submission</h1>

          <p>Submit your assignment via email to clemieux@cs.ubc.ca. Your submission should be a <code>.zip</code> or
            <code>.tar.gz</code> archive named <code>[firstname]_[lastname]_CPSC539L_A1.zip</code> (or equivalently,
            <code>[firstname]_[lastname]_CPSC539L_A1.tar.gz</code>).
          </p>

          <p>The archive should contain the following:
          <ul>
            <li> <code>fuzzer.py</code>: your completed fuzzer.py file</li>
            <li> A writeup file in <code>.txt</code>, <code>.pdf</code>, or <code>.md</code> format which:</li>
            <ul>
              <li> Describes your implementation decisions in <code>fuzzer.py</code></li>
              <li> Answers all questions from the experimental component description (1 (a), (b); 2 (a), (b); 3 (a),
                (b),
                (c), (d)) </li>
            </ul>
          </ul>
          </p>

          <h1>Bonus Exploration</h1>

          <p>You do not need to answer these questions to achieve full marks on the assignment.</p>

          <ul>
            <li>Write different implementations of <code>fuzz_prob</code>, <code>num_mutants</code>, and
              <code>mutate_input</code>. Do you get different results than your original implementation?
            </li>
            <li>
              What happens if <code>mutate_input</code> is not non-deterministic, i.e. always mutates the given input in
              a certain way?
            </li>
            <li>Find an interesting self-contained python program (i.e., most functionality in one file). Write a
              <code>test_one_input</code> driver for it and try fuzzing it with the random and coverage-guided fuzzer.
            </li>
            <li>Our implementation keeps track of coverage even for the random fuzzer. In practice, keeping track of
              coverage can reduce the speed of execution. Thus, an advantage of random fuzzing in many scenarios is that
              more inputs can be evaluated than with coverage guidance. If it is relatively easy to find bugs with
              random fuzzing, coverage-guided fuzzing will often be slower at finding initial inputs. If you modify the
              code to remove coverage tracking (<code>self.cov.start()</code> and <code>self.cov.stop()</code> in
              <code>exec_with_coverage</code>), and save only failing inputs, can you find failing inputs faster with
              random fuzzing than coverage-guided fuzzing?
            </li>
          </ul>

          <h3>Note</h3>

          <p>While this is labelled Assignment 1, it is indeed the only assignment for the 2022W1 offering of the
            course. </p>

        </td>
      </tr>



      <script type="text/javascript">

        var m = "Last updated: " + document.lastModified;
        var p = m.length - 8;
        document.writeln('<p id="update">');
        document.write(m.substring(p, 0));
        document.writeln("</p>");

      </script>
</body>

</html>